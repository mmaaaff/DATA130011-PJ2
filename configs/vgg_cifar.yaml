# Network configurations
network:
  name: "VGG_A_BatchNorm"  # Options: DenseNet121, DenseNet169, DenseNet201, DenseNet
  params:
    inp_ch: 3
    num_classes: 10
    
# Training configurations
training:
  epochs: 50
  batch_size: 256
  num_workers: 4
  device: "cuda:0"  # Options: cuda, cpu #note
  seed: 42

# Optimizer configurations
optimizer:
  type: "Adam"  # Options: Adam, SGD
  params:
    lr: 0.0001  #note
    weight_decay: 0.0001
  # scheduler:
  #   type: "CosineAnnealingLR"  # Options: CosineAnnealingLR, StepLR, ReduceLROnPlateau
  #   params:
  #     T_max: 50  # Should match number of epochs
  #     eta_min: 0.0001

# Data configurations
data:
  dataset: "CIFAR10"
  root: "./data"
  train_transforms:
    - name: "RandomCrop"
      params:
        size: 32
        padding: 4
    - name: "RandomHorizontalFlip"
      params:
        p: 0.5
    - name: "ToTensor"
    - name: "Normalize"
      params:
        mean: [0.4914, 0.4822, 0.4465]
        std: [0.2023, 0.1994, 0.2010]
  
  val_transforms:
    - name: "ToTensor"
    - name: "Normalize"
      params:
        mean: [0.4914, 0.4822, 0.4465]
        std: [0.2023, 0.1994, 0.2010]

# Logging configurations
logging:
  save_dir: "./runs"
  save_freq: 10  # Save checkpoint every N epochs
  log_freq: 30  # Log every N steps
  tensorboard: true 