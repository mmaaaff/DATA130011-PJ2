# Network configurations
network:
  type: "resnet"
  name: "ResNet18"  # Options: ResNet18 (BasicBlock), ResNet34 (BasicBlock), ResNet50 (Bottleneck)
  params:
    in_channels: 3
    num_classes: 10
    base_channels: 64
    dropout_rate: 0.1
    zero_init_residual: false
    activation: "GELU"  # Options: ReLU, LeakyReLU, GELU, SiLU(Swish), Mish
    activation_params:  # Parameters for the activation function
      # inplace: true    # For ReLU and LeakyReLU
      # negative_slope: 0.01  # For LeakyReLU

# Training configurations
training:
  epochs: 2
  batch_size: 256
  num_workers: 4
  device: "cuda"  # Options: cuda, cpu
  seed: 42

# Optimizer configurations
optimizer:
  type: "Adam"  # Options: Adam, SGD
  params:
    lr: 0.003
    weight_decay: 0.0001
  scheduler:
    type: "CosineAnnealingLR"  # Options: CosineAnnealingLR, StepLR, ReduceLROnPlateau
    params:
      T_max: 200  # Should match number of epochs
      eta_min: 0.0001

# Data configurations
data:
  dataset: "CIFAR10"
  root: "./data"
  train_transforms:
    - name: "RandomCrop"
      params:
        size: 32
        padding: 4
    - name: "RandomHorizontalFlip"
      params:
        p: 0.5
    - name: "ToTensor"
    - name: "Normalize"
      params:
        mean: [0.4914, 0.4822, 0.4465]
        std: [0.2023, 0.1994, 0.2010]
  
  val_transforms:
    - name: "ToTensor"
    - name: "Normalize"
      params:
        mean: [0.4914, 0.4822, 0.4465]
        std: [0.2023, 0.1994, 0.2010]

# Logging configurations
logging:
  save_dir: "./runs"
  save_freq: 10  # Save checkpoint every N epochs
  log_freq: 100  # Log every N steps
  tensorboard: true 